************************************************************Important************************************************************
1. make Graphs on statistical analysis of wordnet
e.g.
1. Number of nouns, verbs etc.
2. Number of single, multiple phrases in word-net
3. Number of Hypernyms structure in wordNet.


************************************************************Important************************************************************
"StatisticAL SIMILARITY  bin design"

50 % cosine similarity belongs to bin 5
63 % cosine similiarity belongs to bin 7 and so on..

What does this indicates?
We will be able to see how the similarity is mapped across from WordNet to word-embeddings and how this implied shift can bring 
improvement in word-embeddings for the tasks of facet-generation.


************************************************************Important************************************************************

Logic 1

The word vector does not responds to vocabulary that it has not been trained on. For example some scientific words like
"heterotroph" : 

Does not accept multiple words like cross-fertilization, this denotes "", which is a term in itself 

ques 2:

BUt why, do we need to go one level down adding hypernyms and hyponyms ?
Ans: Because, facet generation tasks include not only suggesting facets which lie in the immediate vicinity, rather generating some
specific facets as well, which could/should be generated from those root facets.



************************************************************Important************************************************************
3 diagrams for every startegy design and how they work.

************************************************************Important************************************************************

Further Research Possible
1. do Wordnet Embeddings and then transfer wordnet embeddings to Word2vec.

Two papers include:
1. (Wordnet Embeddings)Saedi, Chakaveh, António Branco, João António Rodrigues and João Ricardo Silva, 2018, "WordNet Embeddings", In Proceedings, 
   3rd Workshop on Representation Learning for Natural Language Processing (RepL4NLP), 56th Annual Meeting of the Association for Computational Linguistics, 15-20 July 2018, Melbourne, Australia.
2. Rajendra Banjade, Nabin Maharjan, Dipesh Gautam, Vasile Rus handling Missing Words by Mapping across word vector representations.

************************************************************Important************************************************************








Understanding similarity distribution among facets in word embeddigs model.
All the experiments are performed for same number of dimensions = 300. 


Are cup or coffee similar or only associated? In a word-embedding model, when asked for facets from a word-embedding 
model both associated words and similar words are suggested, because similarity here is learned through association. But association nevertheless,
does not guarantee similarity.
Even though fasttext has greater accuracy and better similarity distribution favoring facets, when asked for facets, being a 
word-embedding model will return both association and similarity.

Since, there is almost similar Gaussian distribution in different forms of word-embeddings model,
We can not set general threshold of similarity in different models. For example, you cannot say similarity score of 0.7 or greater is a better model
higher which indirectly imply, we can not ask for top N words for facets from word-embeddings model. 

much meaning will be lost with respec

What happens if this universality of similarity expressed through gaussian Distribution is disturbed to 

because all words have unique representation to each other, we risk loss of information hidden among relations between words in word-embedding space, 
because we attempt to move facets to a high density area of space, leaving the remainder
of space sparse. Hence, we distort the relation among words due to such change in representation and end up compromising the efficiency of word-embeddings model.
So, exisiting word-embeddings model are not suitable for facet generation tasks.









Job 

About myslef ?

Hallo, ich heiße Prem Tiwari. Ich komme aus Indien. Gerade studiere ich in der Bauhaus Universität, Weimar. Ich bin kurz vor dem Abschluss meiner Masterarbeit. 
Beruflich, bin ich ein Datawissenschaftler, der sich auf NLP und Time-Series modelle spezialisiert hat. Python ist meine "Mutter-sprache" in programmieren welt. 
ich verstehe jede datawissenschaftliche Aufgabe an sich ein großes Spektrum hat, um Data-Engineering, data-science und Deployment.  
Deschalb habe ich mich am Ende der Datawissenschaft in Beiden Bereichen geschult - allerdings mehr in Data Engineering als Deployment. 
Ich kann arbeite mit Data warehousing technologies wie ETL im Cloud (erfahrung mit google-cloud). 
ich habe mich in jeder Technologie versucht, die es gibt, von parallel cloud computing, über Web-development bis hin zu Block-chain.
Wie sagt man es in Deutsch , "Intangible asset" ? I am your go to guy, when you are thinking about adoption of new technology for your firm. 



"In the last interview, when I mentioned that my thesis uses Wordnet and word embeddings to do IR and SW(semantic web), your reaction was,
that it is theoretical. But I would like to add that in data-science literature, industry and academy are not far apart.

And working with different types of technologies within the NLP segement, gives you a variety of tool that can be put into
action when considering its application to a use-case. So, my experience in both practical aspect(in terms of industry) and theoretical 
aspect (in terms of academics) [IR ] gives me a background to tackle any challenge from the view-point it is offered."


Only difference between academy and industry is the approach. 
I understand that in industry, things progress by version approach. There is an inclination to produce tangible working aspect at earliest and
then improve upon if need be for the better version. Whereas in academy, it can take many forms, one can work on many different aspects of a
single problem simultaneously.
And admitting to your concerns, I understand the workings or process which goes-on in development."


Also, depends how your product is going to be integrated into client system.
	1. if the application is source hungry and data-intensive, and results have to be produced in a swift manner, 
		then development will take time and you would have to hire an engineer in equivalence with google's competency. (and not only 1 and many of them).
	2. if the application is data-intensive but calculations/analysis are to be presented not continuously, say every 15 minutes or more, then 1 engineer is enough 
		and I think for this purpose, I am you guy with this apt amount of skill-set.

1. Difference between a google's data scientist and me :
if we use a scoring system 0 being worst and 10 being best, the task of data-scientist could be divided into two.
	1. Coding [ if a google's data scientist scores 9/10 out of 10, here I will score a 7/8]
	2. Understanding the underlying algorithm and researching new technologies to accommodate the results. [Here its dependent on the tasks and familiarity 
		with variable segments in the data-scientist industry]. [I will score the same as them, because of inherent interests in interdisplinary segemnts
		I search, study and accomodate these results in my day-to-day experience].



I am excited and want to be in this field. I am like a sponge, i absorb knowledge like it at any instance. I have been trying to break in this field but I 
am all over the places because of no seemingly correct direction. If I am at your place, with correct direction you shall see that I am learning 10% faster than 
any of your employee working. Infact, i am ready to this extent, that I will work first 2 months on any salary you see fit and then you can decide for yourself.

I have confidence in myself and I need you to put your confidence in me and see the result yourselves.




 