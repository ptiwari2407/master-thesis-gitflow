{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Noun_synset_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LILslGZn6m2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc58d945-5ec0-447b-cb3b-de06684afefa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import factorial as f\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9n4HCSLpQqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b4ae1bce-17f4-45d0-fbf6-2420f0622e73"
      },
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 96.7% 1607.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7LvM-1noNRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "39defd1c-ed9f-4b2c-f296-ed4d1093026f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w5hQtAaoqfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24662d1a-0858-4183-ffe4-678805c25eef"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "allSynsets = list(wn.all_synsets(wn.NOUN))\n",
        "print(len(allSynsets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KNeXzT2oiro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf733c9-e6a7-4caf-ddc1-d49e27df672f"
      },
      "source": [
        "sub_allSynsets = allSynsets[0:80]\n",
        "print(len(sub_allSynsets))\n",
        "syn_group = []\n",
        "syn_dict = dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n-kELafo7yI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c5b1ab4-6675-4142-b46c-c9b88b183f43"
      },
      "source": [
        "# def unlist(m):\n",
        "#     q = []\n",
        "#     for x in m:\n",
        "#         if(type(x) is list):\n",
        "#             for y in x:\n",
        "#                 q.append(y)\n",
        "#             continue\n",
        "#         q.append(x)\n",
        "#     return q\n",
        "\n",
        "# x = [[1],[2],[3,[4,[6,9,21]]]]\n",
        "# y = [4,5,6]\n",
        "# for i in range(len(y)):\n",
        "#   if(type(y[i]) is not list):\n",
        "#     if (i==(len(y)-1)):\n",
        "#       print(\"hello from here\")\n",
        "#     continue\n",
        "#   print(\"hello\")\n",
        "x= [1,2,3]\n",
        "y = [4,5,6]\n",
        "x+y\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3GgpriJOZ_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b3e32a3-3e07-4af6-b6f6-fa22a2acaefe"
      },
      "source": [
        "# using recursion to solve unlist function\n",
        "def unlist(m):\n",
        "  q = []\n",
        "  p = []\n",
        "  for x in m:\n",
        "    if (type(x) is list):\n",
        "      for y in x:\n",
        "        q.append(y)\n",
        "      continue\n",
        "    q.append(x)\n",
        "  for i in range(len(q)):\n",
        "    if (type(q[i]) is not list):\n",
        "      if (i == (len(q)-1)):\n",
        "        return q\n",
        "      continue\n",
        "    else:\n",
        "      break\n",
        "  for elem in q:\n",
        "    if (type(elem) is list):\n",
        "      z = unlist(elem)\n",
        "      q.remove(elem)\n",
        "      for item in z:\n",
        "        q.append(item)\n",
        "  return q\n",
        "      \n",
        "\n",
        "    \n",
        "x = [[1],[2],[3,[4,[6,9,21]]]]\n",
        "print(unlist(x)) \n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 6, 9, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl400qPTAY6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "syn_group = []\n",
        "syn_dict = dict()\n",
        "synGroup_length = []\n",
        "\n",
        "for elem in sub_allSynsets:\n",
        "  syn_group.append(elem.name())\n",
        "\n",
        "for elem in syn_group:\n",
        "  word = wn.synset(elem)\n",
        "  syn_dict[elem] = word.lemma_names()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU8YVg04DX2a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "965e6eb4-2839-4ef4-bcc3-46284f752b46"
      },
      "source": [
        "single_lemma = dict()\n",
        "multiple_lemma = dict()\n",
        "for key, value in syn_dict.items():\n",
        "  if(len(value)<2):\n",
        "    single_lemma[key] = value\n",
        "  else:\n",
        "    multiple_lemma[key] = value\n",
        "random_list = []\n",
        "\n",
        "for key, value in single_lemma.items():\n",
        "  random_list.append(value)\n",
        "  \n",
        "random_list = unlist(random_list)\n",
        "\n",
        "ohne_phrase =[]\n",
        "for item in random_list:\n",
        "    if('_' not in item):\n",
        "        ohne_phrase.append(item)\n",
        "print(len(ohne_phrase))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JkuAWv_GkDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "f8018966-46e2-4eba-bd21-c6d25ab85b63"
      },
      "source": [
        "multi_lemma_list = []\n",
        "length_per_list = []\n",
        "\n",
        "for key,value in multiple_lemma.items():\n",
        "  multi_lemma_list.append(value)\n",
        "  length_per_list.append(len(value))\n",
        "for item,l in zip(multi_lemma_list, length_per_list) :\n",
        "  print(item, l)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abstraction', 'abstract_entity'] 2\n",
            "['object', 'physical_object'] 2\n",
            "['whole', 'unit'] 2\n",
            "['living_thing', 'animate_thing'] 2\n",
            "['organism', 'being'] 2\n",
            "['causal_agent', 'cause', 'causal_agency'] 3\n",
            "['person', 'individual', 'someone', 'somebody', 'mortal', 'soul'] 6\n",
            "['animal', 'animate_being', 'beast', 'brute', 'creature', 'fauna'] 6\n",
            "['plant', 'flora', 'plant_life'] 3\n",
            "['food', 'nutrient'] 2\n",
            "['artifact', 'artefact'] 2\n",
            "['cognition', 'knowledge', 'noesis'] 3\n",
            "['motivation', 'motive', 'need'] 3\n",
            "['shape', 'form'] 2\n",
            "['space', 'infinite'] 2\n",
            "['process', 'physical_process'] 2\n",
            "['act', 'deed', 'human_action', 'human_activity'] 4\n",
            "['group', 'grouping'] 2\n",
            "['measure', 'quantity', 'amount'] 3\n",
            "['kindness', 'benignity'] 2\n",
            "['abdominoplasty', 'tummy_tuck'] 2\n",
            "['accomplishment', 'achievement'] 2\n",
            "['beachhead', 'foothold'] 2\n",
            "['feat', 'effort', 'exploit'] 3\n",
            "['course', 'course_of_action'] 2\n",
            "['interchange', 'reciprocation', 'give-and-take'] 3\n",
            "['cross-fertilization', 'cross-fertilisation'] 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ldaFNU4mopo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1f89f7a-7930-4778-98b4-719de8481fba"
      },
      "source": [
        "# Now we start by working with multi_lemma and feed the list to algorithm\n",
        "# Few lemmas have 2,3 or more lemma names for a particular list and you have to work with that\n",
        "# Some combinations have to be used in implementation\n",
        "def combination(n):\n",
        "  return f(n) // f(2) // f(n-2)\n",
        "\n",
        "total_runs = sum(map(combination, length_per_list))\n",
        "np.random.seed(412)\n",
        "r_index = np.random.randint(low=0, high= len(ohne_phrase), size= total_runs)\n",
        "ohne_phrase = pd.Series(ohne_phrase)\n",
        "random_list = list(ohne_phrase[r_index])\n",
        "random_list[6:9]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thing', 'native', 'masterstroke']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16QRG0CWmpSj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d148c15d-d0f5-40e4-9d56-e7cdee4ec240"
      },
      "source": [
        "def generate_combination_pair(m_list):\n",
        "  out_list = []\n",
        "  for elem in m_list:\n",
        "    for i in range(len(elem)):\n",
        "      for j in range(i+1, len(elem)):\n",
        "        out_list.append([elem[i], elem[j]])\n",
        "  return out_list\n",
        "\n",
        "total_pairs = generate_combination_pair(multi_lemma_list)\n",
        "len(total_pairs) == len(random_list)\n",
        "# total_pairs[5:8]\n",
        "total_pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['abstraction', 'abstract_entity'],\n",
              " ['object', 'physical_object'],\n",
              " ['whole', 'unit'],\n",
              " ['living_thing', 'animate_thing'],\n",
              " ['organism', 'being'],\n",
              " ['causal_agent', 'cause'],\n",
              " ['causal_agent', 'causal_agency'],\n",
              " ['cause', 'causal_agency'],\n",
              " ['person', 'individual'],\n",
              " ['person', 'someone'],\n",
              " ['person', 'somebody'],\n",
              " ['person', 'mortal'],\n",
              " ['person', 'soul'],\n",
              " ['individual', 'someone'],\n",
              " ['individual', 'somebody'],\n",
              " ['individual', 'mortal'],\n",
              " ['individual', 'soul'],\n",
              " ['someone', 'somebody'],\n",
              " ['someone', 'mortal'],\n",
              " ['someone', 'soul'],\n",
              " ['somebody', 'mortal'],\n",
              " ['somebody', 'soul'],\n",
              " ['mortal', 'soul'],\n",
              " ['animal', 'animate_being'],\n",
              " ['animal', 'beast'],\n",
              " ['animal', 'brute'],\n",
              " ['animal', 'creature'],\n",
              " ['animal', 'fauna'],\n",
              " ['animate_being', 'beast'],\n",
              " ['animate_being', 'brute'],\n",
              " ['animate_being', 'creature'],\n",
              " ['animate_being', 'fauna'],\n",
              " ['beast', 'brute'],\n",
              " ['beast', 'creature'],\n",
              " ['beast', 'fauna'],\n",
              " ['brute', 'creature'],\n",
              " ['brute', 'fauna'],\n",
              " ['creature', 'fauna'],\n",
              " ['plant', 'flora'],\n",
              " ['plant', 'plant_life'],\n",
              " ['flora', 'plant_life'],\n",
              " ['food', 'nutrient'],\n",
              " ['artifact', 'artefact'],\n",
              " ['cognition', 'knowledge'],\n",
              " ['cognition', 'noesis'],\n",
              " ['knowledge', 'noesis'],\n",
              " ['motivation', 'motive'],\n",
              " ['motivation', 'need'],\n",
              " ['motive', 'need'],\n",
              " ['shape', 'form'],\n",
              " ['space', 'infinite'],\n",
              " ['process', 'physical_process'],\n",
              " ['act', 'deed'],\n",
              " ['act', 'human_action'],\n",
              " ['act', 'human_activity'],\n",
              " ['deed', 'human_action'],\n",
              " ['deed', 'human_activity'],\n",
              " ['human_action', 'human_activity'],\n",
              " ['group', 'grouping'],\n",
              " ['measure', 'quantity'],\n",
              " ['measure', 'amount'],\n",
              " ['quantity', 'amount'],\n",
              " ['kindness', 'benignity'],\n",
              " ['abdominoplasty', 'tummy_tuck'],\n",
              " ['accomplishment', 'achievement'],\n",
              " ['beachhead', 'foothold'],\n",
              " ['feat', 'effort'],\n",
              " ['feat', 'exploit'],\n",
              " ['effort', 'exploit'],\n",
              " ['course', 'course_of_action'],\n",
              " ['interchange', 'reciprocation'],\n",
              " ['interchange', 'give-and-take'],\n",
              " ['reciprocation', 'give-and-take'],\n",
              " ['cross-fertilization', 'cross-fertilisation']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psunyxxkj7IZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6f917db9-2294-4119-bf1f-53455c0988c6"
      },
      "source": [
        "# Now we have generated all the necessary pairs and the random words for whom the comparison should be run, now we need to feed them to alogrithm\n",
        "# in a way, to generate similarity and we shall assign similiarity to particular bins for analysis later\n",
        "\n",
        "def similarity_analysis():\n",
        "  sep = \"_\"\n",
        "  for x,y in zip(total_pairs, random_list):\n",
        "    if (sep in x[0] and sep in x[1]):\n",
        "      a,b = x[0].split(sep)\n",
        "      c,d = x[1].split(sep)\n",
        "    if (sep in x[0] and sep not in x[1]):\n",
        "      a,b = x[0].split(sep)\n",
        "    if (sep in x[1] and sep not in x[0]):\n",
        "      c,d = x[1].split(sep)\n",
        "    else:\n",
        "      \n",
        "    \n",
        "for x in total_pairs:\n",
        "  sep = \"_\"\n",
        "  if ('_' in x[0]):\n",
        "    a, b = x[0].split(sep,)\n",
        "    print(a,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "living thing\n",
            "causal agent\n",
            "causal agent\n",
            "animate being\n",
            "animate being\n",
            "animate being\n",
            "animate being\n",
            "human action\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNTV5EEWxlsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "1cdc9a66-e803-4774-f353-1ca2e3720473"
      },
      "source": [
        "for x in total_pairs:\n",
        "  sep = [\"_\", \"-\"]\n",
        "  if (sep in x[0] and sep in  x[1]):\n",
        "    a, b = x[0].split(sep,)\n",
        "    print(a,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cf5fd080db5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msep\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFP0ojxsj7yR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5e02f4c3-91e8-427b-e4f9-333e8fae9f93"
      },
      "source": [
        "wv.n_similarity(['cross','fertilization'], ['agric'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0542847"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzPB522kj8O5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odSMchsXDL_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logic 1 : Access only the lemma_names of the synset itself\n",
        "syn_group = []\n",
        "syn_dict = dict()\n",
        "# synGroup_length = []\n",
        "for elem in allSynsets:\n",
        "    syn_group.append(elem.name())\n",
        "\n",
        "for elem in syn_group:\n",
        "    word = wn.synset(elem)\n",
        "    syn_dict[elem] = word.lemma_names()\n",
        "    \n",
        "    \n",
        "\n",
        "# print(len(synGroup_length))\n",
        "# arr = np.array(synGroup_length)\n",
        "# (unique, counts) = np.unique(arr, return_counts=True)\n",
        "# frequencies = np.asarray((unique, counts)).T\n",
        "# # print(frequencies)\n",
        "\n",
        "single_lemma = dict()\n",
        "multiple_lemma = dict()\n",
        "for key, value in syn_dict.items():\n",
        "  if(len(value)<2):\n",
        "    single_lemma[key] = value\n",
        "  else:\n",
        "    multiple_lemma[key] = value\n",
        "\n",
        "# from the single_lemma, generate THE list of random words, which would be completely un-correlated later when we\n",
        "# would need to feed to algorithms some random words\n",
        "random_list = []\n",
        "a= []\n",
        "for key, value in single_lemma.items():\n",
        "  random_list.append(value)\n",
        "  \n",
        "random_list = unlist(random_list)\n",
        "print(random_list[0:40])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aSUnwvxDWCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(multiple_lemma))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fSceHOzDfXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logic_list_1 = []\n",
        "for key, value in syn_dict.items():\n",
        "  temp = \",\".join(value)\n",
        "  logic_list_1.append(temp)\n",
        "  print(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0Sc9f8hDeN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/quant/logic-1.txt\", 'w') as filename:\n",
        "  for item in logic_list_1:\n",
        "    filename.write('%s\\n' % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weZaHBiQDb6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  print(logic_list_1[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqpi72EQo-bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for elem in sub_allSynsets:\n",
        "    syn_group.append(elem.name())\n",
        "syn_group = syn_group[14:17]\n",
        "z = ['dog.n.01','frump.n.01','benthos.n.01', 'access.n.04']\n",
        "syn_group.append(z)\n",
        "syn_group = unlist(syn_group)\n",
        "syn_group"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVCfaWDuDaYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY4DIchqpBtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypo = lambda s: s.hyponyms()\n",
        "hyper = lambda s: s.hypernyms()\n",
        "\n",
        "\"\"\"\n",
        "# the following generalisation applies all the hyponyms embodies the general idea \n",
        "# so, we can get upto 20 depths possible \n",
        "# But the hypernym meaning coincide to a greater degree only one/two depth up and no \n",
        "# more, so if after inclusion of hyponym, the list does not grows more than 5\n",
        "# we seek answers from Hypernym structure\n",
        "# The data-collection from word-net itself is gathered to match the represenation of \n",
        "# of word-net itself\n",
        "\"\"\"\n",
        "\n",
        "for elem in allSynsets:\n",
        "    syn_group.append(elem.name())\n",
        "for elem in syn_group:\n",
        "    word = wn.synset(elem)\n",
        "    syn_dict[elem] = word.lemma_names()\n",
        "    syn_dict[elem] = unlist(syn_dict[elem])\n",
        "    hypo_list = list(set(word.closure(hypo, depth=20)))\n",
        "    hypo_list = hypo_list[0:10]\n",
        "    \n",
        "    for item in hypo_list:\n",
        "        temp =[]\n",
        "        name = item.name()\n",
        "        temp = wn.synset(name).lemma_names()\n",
        "        syn_dict[elem].append(temp)  \n",
        "    syn_dict[elem] = unlist(syn_dict[elem])\n",
        "    \n",
        "    a=len(syn_dict[elem])\n",
        "    if(a>5):\n",
        "        continue\n",
        "    \n",
        "    hyper_list = list(set(word.closure(hyper, depth=1)))\n",
        "    \n",
        "    for item in hyper_list:\n",
        "        temp =[]\n",
        "        name = item.name()\n",
        "        temp = wn.synset(name).lemma_names()\n",
        "        syn_dict[elem].append(temp)  \n",
        "    \n",
        "    \n",
        "    for item in hyper_list:\n",
        "        name = item.name()\n",
        "        w1 = wn.synset(name)\n",
        "        w1_list = list(set(w1.closure(hypo, depth=1)))\n",
        "        for id in w1_list:\n",
        "          temp=[]\n",
        "          temp.append(id.lemma_names())\n",
        "          syn_dict[elem].append(temp)\n",
        "        # syn_dict[elem] = unlist(syn_dict[elem])\n",
        "    syn_dict[elem] = unlist(syn_dict[elem])\n",
        "    syn_dict[elem] = unlist(syn_dict[elem])\n",
        "    syn_dict[elem] = unlist(syn_dict[elem])\n",
        "    print(syn_dict[elem])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0DpEuxZpXEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(syn_dict.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG1euMAP3MTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import json\n",
        "# json.dump(syn_dict, open(\"syn_dict.json\",'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GhNm5pA5itl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we shall work with extraction of Hypernym-hyponym set."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}