{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyponym-set-extraction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GRoZbZqiHno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "665dd2ff-03fe-4fe2-cee1-31496c93dbe1"
      },
      "source": [
        "# this script makes up data by going one level down: top-down traversal\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "\n",
        "allSynsets = list(wn.all_synsets(wn.NOUN))\n",
        "sub_allSynsets = allSynsets[0:40]\n",
        "\n",
        "# Multi-level flattening of list using recursion\n",
        "flatten=lambda l: sum(map(flatten,l),[]) if isinstance(l,list) else [l]\n",
        "\n",
        "temp = list()\n",
        "syn_group = list()\n",
        "syn_dict = dict()\n",
        "hypo = lambda s: s.hyponyms()\n",
        "for elem in allSynsets:\n",
        "    syn = elem.name()\n",
        "    syn_dict[syn] = elem.lemma_names()\n",
        "    hypo_syn_list = list(elem.closure(hypo, depth=1))\n",
        "    for item in hypo_syn_list:\n",
        "      temp = syn_dict[syn]\n",
        "      syn_dict[syn] = temp + item.lemma_names()\n",
        "    syn_group.append(flatten(syn_dict[syn]))\n",
        "\n",
        "temp = list()\n",
        "for item in syn_group:\n",
        "  temp.append(len(item))\n",
        "\n",
        "\n",
        "def CountFrequency(my_list):\n",
        "    # Creating an empty dictionary\n",
        "    freq = {}\n",
        "    for item in my_list:\n",
        "        if (item in freq):\n",
        "            freq[item] += 1\n",
        "        else:\n",
        "            freq[item] = 1\n",
        "    print(len(freq))\n",
        "    freq = dict(sorted(freq.items()))\n",
        "\n",
        "    for key, value in freq.items():\n",
        "        print(\"% d : % d\" % (key, value))\n",
        "\n",
        "\n",
        "CountFrequency(temp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "165\n",
            " 1 :  33882\n",
            " 2 :  22951\n",
            " 3 :  9044\n",
            " 4 :  4568\n",
            " 5 :  2578\n",
            " 6 :  1734\n",
            " 7 :  1245\n",
            " 8 :  920\n",
            " 9 :  728\n",
            " 10 :  543\n",
            " 11 :  426\n",
            " 12 :  378\n",
            " 13 :  332\n",
            " 14 :  271\n",
            " 15 :  270\n",
            " 16 :  201\n",
            " 17 :  204\n",
            " 18 :  142\n",
            " 19 :  150\n",
            " 20 :  117\n",
            " 21 :  106\n",
            " 22 :  113\n",
            " 23 :  89\n",
            " 24 :  78\n",
            " 25 :  73\n",
            " 26 :  68\n",
            " 27 :  66\n",
            " 28 :  46\n",
            " 29 :  49\n",
            " 30 :  41\n",
            " 31 :  37\n",
            " 32 :  38\n",
            " 33 :  23\n",
            " 34 :  25\n",
            " 35 :  31\n",
            " 36 :  24\n",
            " 37 :  23\n",
            " 38 :  30\n",
            " 39 :  19\n",
            " 40 :  22\n",
            " 41 :  15\n",
            " 42 :  12\n",
            " 43 :  18\n",
            " 44 :  22\n",
            " 45 :  18\n",
            " 46 :  20\n",
            " 47 :  10\n",
            " 48 :  20\n",
            " 49 :  8\n",
            " 50 :  5\n",
            " 51 :  8\n",
            " 52 :  5\n",
            " 53 :  9\n",
            " 54 :  12\n",
            " 55 :  9\n",
            " 56 :  8\n",
            " 57 :  9\n",
            " 58 :  6\n",
            " 59 :  8\n",
            " 60 :  4\n",
            " 61 :  5\n",
            " 62 :  9\n",
            " 63 :  4\n",
            " 64 :  4\n",
            " 65 :  8\n",
            " 66 :  5\n",
            " 67 :  5\n",
            " 68 :  5\n",
            " 69 :  2\n",
            " 70 :  2\n",
            " 71 :  6\n",
            " 72 :  4\n",
            " 73 :  3\n",
            " 74 :  4\n",
            " 75 :  7\n",
            " 76 :  6\n",
            " 77 :  1\n",
            " 78 :  4\n",
            " 79 :  3\n",
            " 80 :  2\n",
            " 82 :  2\n",
            " 83 :  2\n",
            " 84 :  3\n",
            " 85 :  1\n",
            " 86 :  2\n",
            " 87 :  6\n",
            " 90 :  1\n",
            " 91 :  1\n",
            " 92 :  3\n",
            " 93 :  1\n",
            " 94 :  2\n",
            " 96 :  1\n",
            " 98 :  1\n",
            " 99 :  3\n",
            " 100 :  4\n",
            " 102 :  1\n",
            " 105 :  1\n",
            " 106 :  2\n",
            " 107 :  1\n",
            " 108 :  1\n",
            " 109 :  1\n",
            " 110 :  3\n",
            " 111 :  2\n",
            " 115 :  1\n",
            " 116 :  2\n",
            " 117 :  2\n",
            " 119 :  1\n",
            " 121 :  2\n",
            " 123 :  1\n",
            " 127 :  2\n",
            " 129 :  1\n",
            " 133 :  2\n",
            " 135 :  1\n",
            " 136 :  1\n",
            " 137 :  1\n",
            " 142 :  1\n",
            " 143 :  2\n",
            " 145 :  2\n",
            " 147 :  1\n",
            " 149 :  1\n",
            " 154 :  1\n",
            " 155 :  1\n",
            " 156 :  1\n",
            " 159 :  1\n",
            " 163 :  1\n",
            " 165 :  1\n",
            " 168 :  1\n",
            " 172 :  1\n",
            " 174 :  1\n",
            " 178 :  1\n",
            " 181 :  1\n",
            " 187 :  1\n",
            " 189 :  1\n",
            " 192 :  1\n",
            " 195 :  1\n",
            " 200 :  1\n",
            " 202 :  1\n",
            " 211 :  1\n",
            " 215 :  1\n",
            " 222 :  1\n",
            " 234 :  1\n",
            " 238 :  1\n",
            " 252 :  1\n",
            " 255 :  1\n",
            " 259 :  1\n",
            " 266 :  1\n",
            " 272 :  1\n",
            " 295 :  2\n",
            " 354 :  1\n",
            " 370 :  1\n",
            " 388 :  1\n",
            " 403 :  1\n",
            " 430 :  1\n",
            " 443 :  1\n",
            " 473 :  1\n",
            " 503 :  1\n",
            " 563 :  1\n",
            " 572 :  1\n",
            " 585 :  1\n",
            " 594 :  1\n",
            " 645 :  1\n",
            " 697 :  1\n",
            " 772 :  1\n",
            " 779 :  1\n",
            " 927 :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-L9G9DViXzE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "1977eae1-e634-4f98-e6cb-63c583586a86"
      },
      "source": [
        "for item in syn_group[1:600]:\n",
        "  if (len(item) == 4):\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['contact', 'brush', 'eye_contact', 'placement']\n",
            "['running_away', 'elopement', 'escape', 'flight']\n",
            "['stunt', 'acrobatic_stunt', 'acrobatic_feat', 'Russian_roulette']\n",
            "['penetration', 'interpenetration', 'permeation', 'market_penetration']\n",
            "['docking', 'moorage', 'dockage', 'tying_up']\n",
            "['evacuation', 'medical_evacuation', 'medevac', 'medivac']\n",
            "['retreat', 'disengagement', 'fallback', 'pullout']\n",
            "['debarkation', 'disembarkation', 'disembarkment', 'going_ashore']\n",
            "['dispatch', 'despatch', 'shipment', 'reshipment']\n",
            "['bell_ringer', \"bull's_eye\", 'mark', 'home_run']\n",
            "['credit', 'course_credit', 'semester_hour', 'credit_hour']\n",
            "['ballup', 'balls-up', 'cockup', 'mess-up']\n",
            "['trip', 'trip-up', 'stumble', 'misstep']\n",
            "['conquest', 'conquering', 'subjection', 'subjugation']\n",
            "['abandonment', 'discard', 'discard', 'throwing_away']\n",
            "['capitalization', 'capitalisation', 'overcapitalization', 'overcapitalisation']\n",
            "['conversion', 'rebirth', 'spiritual_rebirth', 'proselytism']\n",
            "['service', 'curb_service', 'self-service', 'valet_parking']\n",
            "['launching', 'launch', 'rocket_firing', 'rocket_launching']\n",
            "['change-up', 'change-of-pace', 'change-of-pace_ball', 'off-speed_pitch']\n",
            "['curve', 'curve_ball', 'breaking_ball', 'bender']\n",
            "['dunk', 'dunk_shot', 'stuff_shot', 'slam_dunk']\n",
            "['shove', 'bundling', 'jostle', 'jostling']\n",
            "['sending', 'transmission', 'transmittal', 'transmitting']\n",
            "['referral', 'remission', 'remitment', 'remit']\n",
            "['shoot', 'skeet', 'skeet_shooting', 'trapshooting']\n",
            "['discharge', 'firing', 'firing_off', 'gun']\n",
            "['crash', 'smash', 'impingement', 'impaction']\n",
            "['force_out', 'force-out', 'force_play', 'force']\n",
            "['homer', 'home_run', 'solo_homer', 'solo_blast']\n",
            "['single', 'bingle', 'line-drive_single', 'line_single']\n",
            "['smack', 'smacking', 'slap', 'spank']\n",
            "['haymaker', 'knockout_punch', 'KO_punch', 'Sunday_punch']\n",
            "['place_kick', 'place-kicking', 'free_kick', 'kickoff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWih58uQidFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "bc6bf147-892a-40d0-9d57-c27fc8a198b4"
      },
      "source": [
        "# this script makes up data by going one level up: bottom-up traversal\n",
        "\n",
        "\n",
        "allSynsets = list(wn.all_synsets(wn.NOUN))\n",
        "sub_allSynsets = allSynsets[0:40]\n",
        "\n",
        "# Multi-level flattening of list using recursion\n",
        "flatten=lambda l: sum(map(flatten,l),[]) if isinstance(l,list) else [l]\n",
        "\n",
        "temp = list()\n",
        "syn_group = list()\n",
        "syn_dict = dict()\n",
        "hyper = lambda s: s.hypernyms()\n",
        "for elem in allSynsets:\n",
        "    syn = elem.name()\n",
        "    syn_dict[syn] = elem.lemma_names()\n",
        "    hyper_syn_list = list(elem.closure(hyper, depth=1))\n",
        "    for item in hyper_syn_list:\n",
        "      temp = syn_dict[syn]\n",
        "      syn_dict[syn] = temp + item.lemma_names()\n",
        "    syn_group.append(flatten(syn_dict[syn]))\n",
        "\n",
        "temp = list()\n",
        "for item in syn_group:\n",
        "  temp.append(len(item))\n",
        "\n",
        "CountFrequency(temp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\n",
            " 1 :  2780\n",
            " 2 :  20921\n",
            " 3 :  24507\n",
            " 4 :  14697\n",
            " 5 :  8866\n",
            " 6 :  4726\n",
            " 7 :  2764\n",
            " 8 :  1369\n",
            " 9 :  679\n",
            " 10 :  340\n",
            " 11 :  211\n",
            " 12 :  80\n",
            " 13 :  75\n",
            " 14 :  32\n",
            " 15 :  25\n",
            " 16 :  12\n",
            " 17 :  11\n",
            " 18 :  4\n",
            " 19 :  5\n",
            " 20 :  4\n",
            " 21 :  1\n",
            " 22 :  1\n",
            " 23 :  1\n",
            " 28 :  1\n",
            " 29 :  2\n",
            " 31 :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV9iAUuYkHRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "a93be04b-6742-47cd-cd12-2878fb54aa3e"
      },
      "source": [
        "dog = wn.synset('working_dog.n.01')\n",
        "hypo = lambda s: s.hyponyms()\n",
        "hyper = lambda s: s.hypernyms()\n",
        "z = list(set(dog.closure(hypo, depth=1)))\n",
        "# print(\"*****\")\n",
        "# z = list(dog.closure(hyper, depth=1))\n",
        "print(z)\n",
        "count = 0\n",
        "for item in z:\n",
        "  count = count+1\n",
        "  # print(count)\n",
        "  print(item.lemma_names())\n",
        "# print(list(dog.closure(hypo, depth=20)))\n",
        "# print(list(dog.closure(hyper, depth=2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('guide_dog.n.01'), Synset('eskimo_dog.n.01'), Synset('seizure-alert_dog.n.01'), Synset('sennenhunde.n.01'), Synset('hearing_dog.n.01'), Synset('great_dane.n.01'), Synset('boxer.n.04'), Synset('police_dog.n.01'), Synset('watchdog.n.02'), Synset('bull_mastiff.n.01'), Synset('mastiff.n.01'), Synset('sled_dog.n.01'), Synset('bulldog.n.01'), Synset('shepherd_dog.n.01'), Synset('saint_bernard.n.01')]\n",
            "['guide_dog']\n",
            "['Eskimo_dog', 'husky']\n",
            "['seizure-alert_dog']\n",
            "['Sennenhunde']\n",
            "['hearing_dog']\n",
            "['Great_Dane']\n",
            "['boxer']\n",
            "['police_dog']\n",
            "['watchdog', 'guard_dog']\n",
            "['bull_mastiff']\n",
            "['mastiff']\n",
            "['sled_dog', 'sledge_dog']\n",
            "['bulldog', 'English_bulldog']\n",
            "['shepherd_dog', 'sheepdog', 'sheep_dog']\n",
            "['Saint_Bernard', 'St_Bernard']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSkfRKofkeaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "356565cb-8397-4c7e-8c84-a972c6de9857"
      },
      "source": [
        "dog = wn.synset()\n",
        "print(dog.lemma_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dog', 'domestic_dog', 'Canis_familiaris']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVNfX0x4m1ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similarity_check_1(pair, rand_list, pos=0):\n",
        "    \"\"\"\n",
        "    1. This performs cosine similarity analysis between 1st word of the pair as support and 2nd word of pair as queried : True\n",
        "    2. This performs cosine similarity analysis between 1st word of the pair and random word queried as False\n",
        "    3. word_not_in_vocab\n",
        "    \"\"\"\n",
        "    word_not_in_vocab = list()\n",
        "    correct_pred = list()\n",
        "    false_pred = list()\n",
        "\n",
        "    for x,y in zip(pair, rand_list):\n",
        "        support = x[pos]                  #Given word\n",
        "        query_T = x[1-pos]                  #True Queried\n",
        "        query_F = y                     #False Queried\n",
        "\n",
        "        if (support in wv.vocab and query_T in wv.vocab):\n",
        "            syn_sim = wv.n_similarity([support], [query_T])\n",
        "            rand_sim = wv.n_similarity([support], [query_F])\n",
        "            if(syn_sim > rand_sim):\n",
        "                correct_pred.append(syn_sim)\n",
        "            else:\n",
        "                false_pred.append(rand_sim)\n",
        "        else:\n",
        "            word_not_in_vocab.append(x)\n",
        "\n",
        "    return(word_not_in_vocab, correct_pred, false_pred)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MU1jkEuo8xU",
        "colab_type": "text"
      },
      "source": [
        "### Things to be discussed with Tim\n",
        "\n",
        "\n",
        "\n",
        "1.  Two types of tree traversal - Hypernym tree traversal and Hyponym tree traversal.\n",
        "2.   Combination pair geneartion for analysis.\n",
        "3.  N-similarity vector analysis wv.n_similarity(['sushi', 'restraunt'], ['restraunt', 'sushi'])\n",
        "4.  Most-similar function in word vector for multiple words to be fed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dRH5G7_pBHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv.n_similarity(['dog','domestic'], ['canis', 'familairis'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}